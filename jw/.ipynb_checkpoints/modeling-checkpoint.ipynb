{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost, ligthgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocess_X = pd.read_csv('preprocessed_data//preprocess_X_data.csv',index_col=0)\n",
    "preprocess_Y = pd.read_csv('preprocessed_data//preprocess_Y_data.csv',index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m get_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'"
     ]
    }
   ],
   "source": [
    "from pycaret.datasets import get_data\n",
    "dataset = get_data('credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycaret\n",
      "  Using cached pycaret-3.0.2-py3-none-any.whl (483 kB)\n",
      "Requirement already satisfied: ipython>=5.5.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (8.12.0)\n",
      "Requirement already satisfied: ipywidgets>=7.6.5 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (8.0.4)\n",
      "Requirement already satisfied: tqdm>=4.62.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (4.65.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.21 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (1.23.5)\n",
      "Collecting pandas<2.0.0,>=1.3.0 (from pycaret)\n",
      "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "Requirement already satisfied: jinja2>=1.2 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (3.1.2)\n",
      "Requirement already satisfied: scipy<2.0.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (1.2.2)\n",
      "Collecting pyod>=1.0.8 (from pycaret)\n",
      "  Using cached pyod-1.0.9.tar.gz (149 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting imbalanced-learn>=0.8.1 (from pycaret)\n",
      "  Using cached imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "Collecting category-encoders>=2.4.0 (from pycaret)\n",
      "  Using cached category_encoders-2.6.1-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: lightgbm>=3.0.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (3.3.5)\n",
      "Collecting numba>=0.55.0 (from pycaret)\n",
      "  Using cached numba-0.57.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (2.29.0)\n",
      "Requirement already satisfied: psutil>=5.9.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (5.9.0)\n",
      "Requirement already satisfied: markupsafe>=2.0.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (2.1.1)\n",
      "Collecting importlib-metadata>=4.12.0 (from pycaret)\n",
      "  Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (5.7.0)\n",
      "Collecting cloudpickle (from pycaret)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting deprecation>=2.1.0 (from pycaret)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting xxhash (from pycaret)\n",
      "  Using cached xxhash-3.2.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pycaret) (3.7.1)\n",
      "Collecting scikit-plot>=0.3.7 (from pycaret)\n",
      "  Using cached scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Collecting yellowbrick>=1.4 (from pycaret)\n",
      "  Using cached yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "Collecting plotly>=5.0.0 (from pycaret)\n",
      "  Using cached plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n",
      "Collecting kaleido>=0.2.1 (from pycaret)\n",
      "  Using cached kaleido-0.2.1-py2.py3-none-win_amd64.whl (65.9 MB)\n",
      "Collecting schemdraw==0.15 (from pycaret)\n",
      "  Using cached schemdraw-0.15-py3-none-any.whl (106 kB)\n",
      "Collecting plotly-resampler>=0.8.3.1 (from pycaret)\n",
      "  Using cached plotly_resampler-0.8.3.2.tar.gz (46 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting statsmodels>=0.12.1 (from pycaret)\n",
      "  Using cached statsmodels-0.14.0-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "Collecting sktime!=0.17.1,<0.17.2,>=0.16.1 (from pycaret)\n",
      "  Using cached sktime-0.17.0-py3-none-any.whl (16.1 MB)\n",
      "Collecting tbats>=1.1.3 (from pycaret)\n",
      "  Using cached tbats-1.1.3-py3-none-any.whl (44 kB)\n",
      "Collecting pmdarima!=1.8.1,<3.0.0,>=1.8.0 (from pycaret)\n",
      "  Using cached pmdarima-2.0.3-cp311-cp311-win_amd64.whl (566 kB)\n",
      "Collecting patsy>=0.5.1 (from category-encoders>=2.4.0->pycaret)\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from deprecation>=2.1.0->pycaret) (23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from imbalanced-learn>=0.8.1->pycaret) (3.1.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.12.0->pycaret)\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.4.6)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (6.19.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (3.0.5)\n",
      "Requirement already satisfied: wheel in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from lightgbm>=3.0.0->pycaret) (0.38.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (2.8.2)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (5.3.0)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.55.0->pycaret)\n",
      "  Using cached llvmlite-0.40.1rc1-cp311-cp311-win_amd64.whl (27.7 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\envs\\py\\lib\\site-packages (from pandas<2.0.0,>=1.3.0->pycaret) (2022.7)\n",
      "Collecting tenacity>=6.2.0 (from plotly>=5.0.0->pycaret)\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting dash<3.0.0,>=2.2.0 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Using cached dash-2.10.2-py3-none-any.whl (10.3 MB)\n",
      "Collecting jupyter-dash>=0.4.2 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Using cached jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\n",
      "INFO: pip is looking at multiple versions of plotly-resampler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting plotly-resampler>=0.8.3.1 (from pycaret)\n",
      "  Using cached plotly-resampler-0.8.3.1.tar.gz (68 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting Flask-Cors<4.0.0,>=3.0.10 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting Werkzeug<=2.1.2 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "Collecting pandas<2.0.0,>=1.3.0 (from pycaret)\n",
      "  Using cached pandas-1.5.2-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "  Using cached pandas-1.5.1-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "  Using cached pandas-1.5.0-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "INFO: pip is looking at multiple versions of plotly-resampler to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pandas-1.4.4.tar.gz (4.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached pandas-1.4.3.tar.gz (4.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached pandas-1.4.2.tar.gz (4.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached pandas-1.4.1.tar.gz (4.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached pandas-1.4.0.tar.gz (4.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached pandas-1.3.5.tar.gz (4.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached pandas-1.3.4.tar.gz (4.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached pandas-1.3.3.tar.gz (4.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [239 lines of output]\n",
      "  Ignoring numpy: markers 'python_version == \"3.7\" and (platform_machine != \"arm64\" or platform_system != \"Darwin\") and platform_machine != \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\" and (platform_machine != \"arm64\" or platform_system != \"Darwin\") and platform_machine != \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.7\" and platform_machine == \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\" and platform_machine == \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\" and platform_machine == \"arm64\" and platform_system == \"Darwin\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.9\" and platform_machine == \"arm64\" and platform_system == \"Darwin\"' don't match your environment\n",
      "  Collecting setuptools>=51.0.0\n",
      "    Using cached setuptools-67.8.0-py3-none-any.whl (1.1 MB)\n",
      "  Collecting wheel\n",
      "    Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "  Collecting Cython<3,>=0.29.21\n",
      "    Using cached Cython-0.29.35-py2.py3-none-any.whl (988 kB)\n",
      "  Collecting numpy==1.19.3\n",
      "    Using cached numpy-1.19.3.zip (7.3 MB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing metadata (pyproject.toml): started\n",
      "    Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    Preparing metadata (pyproject.toml) did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    [202 lines of output]\n",
      "    setup.py:67: RuntimeWarning: NumPy 1.19.3 may not yet support Python 3.11.\n",
      "      warnings.warn(\n",
      "    Running from numpy source directory.\n",
      "    setup.py:480: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
      "      run_build = parse_setuppy_commands()\n",
      "    Processing numpy/random\\_bounded_integers.pxd.in\n",
      "    Processing numpy/random\\bit_generator.pyx\n",
      "    Processing numpy/random\\mtrand.pyx\n",
      "    Processing numpy/random\\_bounded_integers.pyx.in\n",
      "    Processing numpy/random\\_common.pyx\n",
      "    Processing numpy/random\\_generator.pyx\n",
      "    Processing numpy/random\\_mt19937.pyx\n",
      "    Processing numpy/random\\_pcg64.pyx\n",
      "    Processing numpy/random\\_philox.pyx\n",
      "    Processing numpy/random\\_sfc64.pyx\n",
      "    Cythonizing sources\n",
      "    blas_opt_info:\n",
      "    blas_mkl_info:\n",
      "    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "    customize MSVCCompiler\n",
      "      libraries mkl_rt not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    blis_info:\n",
      "      libraries blis not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    openblas_info:\n",
      "      libraries openblas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
      "    customize GnuFCompiler\n",
      "    Could not locate executable g77\n",
      "    Could not locate executable f77\n",
      "    customize IntelVisualFCompiler\n",
      "    Could not locate executable ifort\n",
      "    Could not locate executable ifl\n",
      "    customize AbsoftFCompiler\n",
      "    Could not locate executable f90\n",
      "    customize CompaqVisualFCompiler\n",
      "    Could not locate executable DF\n",
      "    customize IntelItaniumVisualFCompiler\n",
      "    Could not locate executable efl\n",
      "    customize Gnu95FCompiler\n",
      "    Could not locate executable gfortran\n",
      "    Could not locate executable f95\n",
      "    customize G95FCompiler\n",
      "    Could not locate executable g95\n",
      "    customize IntelEM64VisualFCompiler\n",
      "    customize IntelEM64TFCompiler\n",
      "    Could not locate executable efort\n",
      "    Could not locate executable efc\n",
      "    customize PGroupFlangCompiler\n",
      "    Could not locate executable flang\n",
      "    don't know how to compile Fortran code on platform 'nt'\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_blas_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries tatlas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_blas_info:\n",
      "      libraries satlas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_blas_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries ptf77blas,ptcblas,atlas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_blas_info:\n",
      "      libraries f77blas,cblas,atlas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    accelerate_info:\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-i8rb0h32\\numpy_aeaa55bc3cf649acb7bac9d03e4f29b2\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "        Optimized (vendor) Blas libraries are not found.\n",
      "        Falls back to netlib Blas library which has worse performance.\n",
      "        A better performance should be easily gained by switching\n",
      "        Blas library.\n",
      "      if self._calc_info(blas):\n",
      "    blas_info:\n",
      "      libraries blas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-i8rb0h32\\numpy_aeaa55bc3cf649acb7bac9d03e4f29b2\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "        Blas (http://www.netlib.org/blas/) libraries not found.\n",
      "        Directories to search for the libraries can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [blas]) or by setting\n",
      "        the BLAS environment variable.\n",
      "      if self._calc_info(blas):\n",
      "    blas_src_info:\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-i8rb0h32\\numpy_aeaa55bc3cf649acb7bac9d03e4f29b2\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "        Blas (http://www.netlib.org/blas/) sources not found.\n",
      "        Directories to search for the sources can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [blas_src]) or by setting\n",
      "        the BLAS_SRC environment variable.\n",
      "      if self._calc_info(blas):\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    non-existing path in 'numpy\\\\distutils': 'site.cfg'\n",
      "    lapack_opt_info:\n",
      "    lapack_mkl_info:\n",
      "      libraries mkl_rt not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    openblas_lapack_info:\n",
      "      libraries openblas not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    openblas_clapack_info:\n",
      "      libraries openblas,lapack not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    flame_info:\n",
      "      libraries flame not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries tatlas,tatlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries tatlas,tatlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "      libraries tatlas,tatlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_info:\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries satlas,satlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries satlas,satlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "      libraries satlas,satlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries ptf77blas,ptcblas,atlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "      libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_info:\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries f77blas,cblas,atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries f77blas,cblas,atlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "      libraries f77blas,cblas,atlas not found in C:\\Users\\user\\miniconda3\\envs\\py\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    lapack_info:\n",
      "      libraries lapack not found in ['C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\py\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-i8rb0h32\\numpy_aeaa55bc3cf649acb7bac9d03e4f29b2\\numpy\\distutils\\system_info.py:1748: UserWarning:\n",
      "        Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
      "        Directories to search for the libraries can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
      "        the LAPACK environment variable.\n",
      "      return getattr(self, '_calc_info_{}'.format(name))()\n",
      "    lapack_src_info:\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-i8rb0h32\\numpy_aeaa55bc3cf649acb7bac9d03e4f29b2\\numpy\\distutils\\system_info.py:1748: UserWarning:\n",
      "        Lapack (http://www.netlib.org/lapack/) sources not found.\n",
      "        Directories to search for the sources can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
      "        the LAPACK_SRC environment variable.\n",
      "      return getattr(self, '_calc_info_{}'.format(name))()\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    numpy_linalg_lapack_lite:\n",
      "      FOUND:\n",
      "        language = c\n",
      "        define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]\n",
      "  \n",
      "    C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-env-irmes_wk\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:275: UserWarning: Unknown distribution option: 'define_macros'\n",
      "      warnings.warn(msg)\n",
      "    running dist_info\n",
      "    running build_src\n",
      "    build_src\n",
      "    building py_modules sources\n",
      "    creating build\n",
      "    creating build\\src.win-amd64-3.11\n",
      "    creating build\\src.win-amd64-3.11\\numpy\n",
      "    creating build\\src.win-amd64-3.11\\numpy\\distutils\n",
      "    building library \"npymath\" sources\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    [end of output]\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  error: metadata-generation-failed\n",
      "  \n",
      "  Encountered error while generating package metadata.\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This is an issue with the package mentioned above, not pip.\n",
      "  hint: See above for details.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install pycaret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.3 | packaged by Anaconda, Inc. | (main, Apr 19 2023, 23:46:34) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# print(\"--sys.version—“)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train / Test 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test , y_train , y_test = train_test_split(preprocess_X,preprocess_Y,test_size=0.2,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45359, 10), (45359, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11340, 10), (11340, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_prd</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>CC</th>\n",
       "      <th>Fuel_CNG</th>\n",
       "      <th>Fuel_Diesel</th>\n",
       "      <th>Fuel_Gasoline</th>\n",
       "      <th>Fuel_Hybrid</th>\n",
       "      <th>Fuel_LPG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>149000</td>\n",
       "      <td>1560</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>156000</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12767</th>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>142</td>\n",
       "      <td>168000</td>\n",
       "      <td>1598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>2004</td>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "      <td>147563</td>\n",
       "      <td>1598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50200</th>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>285900</td>\n",
       "      <td>2148</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37582</th>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>256116</td>\n",
       "      <td>1591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41405</th>\n",
       "      <td>2005</td>\n",
       "      <td>12</td>\n",
       "      <td>116</td>\n",
       "      <td>159000</td>\n",
       "      <td>1364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53264</th>\n",
       "      <td>2010</td>\n",
       "      <td>15</td>\n",
       "      <td>72</td>\n",
       "      <td>184000</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16269</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>144000</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48961</th>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>220000</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45359 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year_prd  Brand  Model  Mileage    CC  Fuel_CNG  Fuel_Diesel  \\\n",
       "9519       2012      2     24   149000  1560         0            1   \n",
       "1568       2008      8      5   156000  1998         0            1   \n",
       "12767      2009     12    142   168000  1598         0            0   \n",
       "5441       2004     16     79   147563  1598         0            0   \n",
       "50200      2008      9     26   285900  2148         0            1   \n",
       "...         ...    ...    ...      ...   ...       ...          ...   \n",
       "37582      2013      7     31   256116  1591         0            0   \n",
       "41405      2005     12    116   159000  1364         0            0   \n",
       "53264      2010     15     72   184000  1390         0            0   \n",
       "16269      2016      1    135   144000  1995         0            1   \n",
       "48961      2006      0      9   220000  1968         0            1   \n",
       "\n",
       "       Fuel_Gasoline  Fuel_Hybrid  Fuel_LPG  \n",
       "9519               0            0         0  \n",
       "1568               0            0         0  \n",
       "12767              1            0         0  \n",
       "5441               1            0         0  \n",
       "50200              0            0         0  \n",
       "...              ...          ...       ...  \n",
       "37582              1            0         0  \n",
       "41405              1            0         0  \n",
       "53264              1            0         0  \n",
       "16269              0            0         0  \n",
       "48961              0            0         0  \n",
       "\n",
       "[45359 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>41.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>22.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12767</th>\n",
       "      <td>24.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>20.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50200</th>\n",
       "      <td>33.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37582</th>\n",
       "      <td>41.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41405</th>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53264</th>\n",
       "      <td>24.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16269</th>\n",
       "      <td>103.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48961</th>\n",
       "      <td>21.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45359 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price\n",
       "9519    41.47\n",
       "1568    22.49\n",
       "12767   24.05\n",
       "5441    20.02\n",
       "50200   33.67\n",
       "...       ...\n",
       "37582   41.47\n",
       "41405   12.87\n",
       "53264   24.70\n",
       "16269  103.87\n",
       "48961   21.97\n",
       "\n",
       "[45359 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost, LightGBM 모델 학습\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Train LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "lgb_predictions = lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([74.38312  ,  4.7204523, 12.005095 , ..., 31.501606 , 20.92103  ,\n",
       "        49.2563   ], dtype=float32),\n",
       " array([99.92053465,  8.2313257 , 14.56413332, ..., 33.63694675,\n",
       "        16.72280755, 51.21196623]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictions, lgb_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "#                            colsample_bytree=0.8, \n",
    "#                            learning_rate=0.3, \n",
    "#                            max_depth=6, \n",
    "#                            n_estimators=100, \n",
    "#                            subsample=0.9, \n",
    "#                            min_child_weight=1,\n",
    "#                           scale_pos_weight=1,\n",
    "#                            seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=300, verbose=False)\n",
    "# y_pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 평가지표 : MAE, MSE, RMSE, r^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Evaluation:\n",
      "MAE: 6.39106543884906\n",
      "MSE: 89.33873092597189\n",
      "RMSE: 9.451916785815028\n",
      "R^2: 0.9263569246601951\n"
     ]
    }
   ],
   "source": [
    "def xgb_regression_eval(y_test,xgb_predictions):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import numpy as np\n",
    "    # Evaluate XGBoost model\n",
    "    xgb_mae = mean_absolute_error(y_test, xgb_predictions)\n",
    "    xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
    "    xgb_rmse = np.sqrt(xgb_mse)\n",
    "    xgb_r2 = r2_score(y_test,xgb_predictions)\n",
    "\n",
    "    print(\"XGBoost Model Evaluation:\")\n",
    "    print(\"MAE:\", xgb_mae)\n",
    "    print(\"MSE:\", xgb_mse)\n",
    "    print(\"RMSE:\", xgb_rmse)\n",
    "    print(\"R^2:\", xgb_r2)\n",
    "    return xgb_mae , xgb_mse , xgb_rmse , xgb_r2\n",
    "xgb_mae , xgb_mse , xgb_rmse , xgb_r2 = xgb_regression_eval(y_test,xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Model Evaluation:\n",
      "MAE: 6.8854659393309285\n",
      "MSE: 100.73873367284332\n",
      "RMSE: 10.036868718521895\n",
      "R^2: 0.9169597544467802\n"
     ]
    }
   ],
   "source": [
    "def lgb_regression_eval(y_test,lgb_predictions):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import numpy as np\n",
    "    # Evaluate LightGBM model\n",
    "    lgb_mae = mean_absolute_error(y_test, lgb_predictions)\n",
    "    lgb_mse = mean_squared_error(y_test, lgb_predictions)\n",
    "    lgb_rmse = np.sqrt(lgb_mse)\n",
    "    lgb_r2 = r2_score(y_test,lgb_predictions)\n",
    "\n",
    "    print(\"\\nLightGBM Model Evaluation:\")\n",
    "    print(\"MAE:\", lgb_mae)\n",
    "    print(\"MSE:\", lgb_mse)\n",
    "    print(\"RMSE:\", lgb_rmse)\n",
    "    print(\"R^2:\", lgb_r2)\n",
    "    return lgb_mae , lgb_mse , lgb_rmse , lgb_r2\n",
    "\n",
    "\n",
    "lgb_mae , lgb_mse , lgb_rmse , lgb_r2 = lgb_regression_eval(y_test,lgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 정답 값과 예측 값의 차이를 절댓값으로 변환한 뒤 합산하여 평균을 구한다.\n",
    "# 특이값이 많은 경우에 주로 사용된다.\n",
    "# 값이 낮을수록 좋다.\n",
    "\n",
    "#장점\n",
    "#직관점임\n",
    "#정답 및 예측 값과 같은 단위를 가짐\n",
    "\n",
    "#단점\n",
    "#실제 정답보다 낮게 예측했는지, 높게 했는지를 파악하기 힘듦\n",
    "#스케일 의존적임(scal dependency): 모델마다 에류 크기가 동일해도 에러율은 동일하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE(Mean Squared Error) = 평균 제곱 오차\n",
    "# 실제 정답 값과 예측 값의 차이를 제곱한 뒤 평균을 구한다.\n",
    "# 값이 낮을수록 좋다.\n",
    "\n",
    "# 장점\n",
    "# 직관적임\n",
    "\n",
    "# 단점\n",
    "# 제곱하기 때문에 1미만의 에러는 작아지고, 그 이상의 에러는 커짐\n",
    "# 실제 정답보다 낮게 예측했는지, 높게 했는지를 파악하기 힘듦\n",
    "# 스케일 의존적임(scal dependency): 모델마다 에류 크기가 동일해도 에러율은 동일하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE(Root Mean Squared Error) = 평균 제곱근 오차\n",
    "# MSE에 루트는 씌워서 에러를 제곱해서 생기는 값의 왜곡이 줄어든다.\n",
    "\n",
    "# 값이 낮을수록 좋다.\n",
    "\n",
    "# 장점\n",
    "# 직관적임\n",
    "\n",
    "# 단점\n",
    "# 제곱하기 때문에 1미만의 에러는 작아지고, 그 이상의 에러는 커짐\n",
    "# 실제 정답보다 낮게 예측했는지, 높게 했는지를 파악하기 힘듦\n",
    "# 스케일 의존적임(scal dependency): 모델마다 에류 크기가 동일해도 에러율은 동일하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score = R squard\n",
    "# 다른 지표(MAE, MSE, RMSE)들은 모델마다 값이 다르기 때문에 절대 값만 보고 선능을 판단하기 어렵다.\n",
    "# R2 score는 상대적인 성능을 나타내기 비교가 쉽다.\n",
    "# 실제 값의 분산 대비 예측값의 분산 비율을 의미한다.\n",
    "# 1에 가까울 수록 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_evals = {'CV':'-','Mae':xgb_mae,'Mse':xgb_mse,'Rmse':xgb_rmse,'R2':xgb_r2}\n",
    "lgb_evals = {'CV':'-','Mae':lgb_mae,'Mse':lgb_mse,'Rmse':lgb_rmse,'R2':lgb_r2}\n",
    "\n",
    "xgb_evals, lgb_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importances (피처중요도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.feature_importances_ , preprocess_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model.feature_importances_ , preprocess_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df_feature_importances = pd.DataFrame({'Features':preprocess_X.columns,'Importances':xgb_model.feature_importances_})\n",
    "lgb_df_feature_importances = pd.DataFrame({'Features':preprocess_X.columns,'Importances':lgb_model.feature_importances_})\n",
    "\n",
    "xgb_df_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df_feature_importances = xgb_df_feature_importances.sort_values('Importances',ascending=False)\n",
    "lgb_df_feature_importances = lgb_df_feature_importances.sort_values('Importances',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x='Importances',y='Features',data=xgb_df_feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Importances',y='Features',data=lgb_df_feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocess_X = pd.read_csv('preprocessed_data//preprocess_X_data.csv',index_col=0)\n",
    "preprocess_Y = pd.read_csv('preprocessed_data//preprocess_Y_data.csv',index_col=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgb k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th fold\n",
      "XGBoost Model Evaluation:\n",
      "MAE: 6.579540178203499\n",
      "MSE: 101.8034208027063\n",
      "RMSE: 10.089768124328046\n",
      "R^2: 0.9151648142744194\n",
      "2th fold\n",
      "XGBoost Model Evaluation:\n",
      "MAE: 6.396600788162446\n",
      "MSE: 93.42757568152079\n",
      "RMSE: 9.665794105065594\n",
      "R^2: 0.9221159616687726\n",
      "3th fold\n",
      "XGBoost Model Evaluation:\n",
      "MAE: 6.442510514221494\n",
      "MSE: 91.88121350518352\n",
      "RMSE: 9.585468872474811\n",
      "R^2: 0.9235696191402784\n",
      "4th fold\n",
      "XGBoost Model Evaluation:\n",
      "MAE: 6.207715462824235\n",
      "MSE: 86.34527442317342\n",
      "RMSE: 9.292215797277494\n",
      "R^2: 0.9264965358150542\n",
      "5th fold\n",
      "XGBoost Model Evaluation:\n",
      "MAE: 6.419688164811673\n",
      "MSE: 92.33811311481733\n",
      "RMSE: 9.60927224688828\n",
      "R^2: 0.9219288113001325\n",
      "Mean MAE : 6.40921102164467\n",
      "Mean MSE : 93.15911950548028\n",
      "Mean RMSE : 9.648503829206845\n",
      "Mean R2 : 0.9218551484397315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "def xgb_exec_kfold(preprocess_X, preprocess_Y, folds=5):\n",
    "    xgb = XGBRegressor(random_state=777)\n",
    "    kfold = KFold(n_splits=folds)\n",
    "    evals = {'MAE': [], 'MSE': [], 'RMSE': [], 'R2': []}\n",
    "    xgb_mean_evals = {}\n",
    "\n",
    "    for iter_count, (train_index, test_index) in enumerate(kfold.split(preprocess_X)):\n",
    "        print(str(iter_count + 1) + \"th fold\")\n",
    "        X_train, X_test = preprocess_X.values[train_index], preprocess_X.values[test_index]\n",
    "        y_train, y_test = preprocess_Y.values[train_index], preprocess_Y.values[test_index]\n",
    "\n",
    "        xgb.fit(X_train, y_train)\n",
    "        xgb_predictions = xgb.predict(X_test)\n",
    "\n",
    "        xgb_mae, xgb_mse, xgb_rmse, xgb_r2 = xgb_regression_eval(y_test, xgb_predictions)\n",
    "\n",
    "        evals['MAE'].append(xgb_mae)\n",
    "        evals['MSE'].append(xgb_mse)\n",
    "        evals['RMSE'].append(xgb_rmse)\n",
    "        evals['R2'].append(xgb_r2)\n",
    "\n",
    "    xgb_mean_evals['CV'] = 'KFOLD = ' + str(folds) + ' (mean)'\n",
    "    for key, value in evals.items():\n",
    "        xgb_mean_evals[key] = np.mean(value)\n",
    "        print(\"Mean\", key, \":\", xgb_mean_evals[key])\n",
    "\n",
    "    return xgb_mean_evals\n",
    "\n",
    "# Call exec_kfold\n",
    "xgb_mean_evals = xgb_exec_kfold(preprocess_X, preprocess_Y, folds=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lgb k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\py\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Model Evaluation:\n",
      "MAE: 6.964194778942695\n",
      "MSE: 110.19827577387953\n",
      "RMSE: 10.497536652657114\n",
      "R^2: 0.9081691841177572\n",
      "2th fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\py\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Model Evaluation:\n",
      "MAE: 6.792891946650087\n",
      "MSE: 100.67551224269712\n",
      "RMSE: 10.033718764381286\n",
      "R^2: 0.916073863660394\n",
      "3th fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\py\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Model Evaluation:\n",
      "MAE: 6.880732255779941\n",
      "MSE: 102.04578341790105\n",
      "RMSE: 10.101771301009594\n",
      "R^2: 0.915114333015216\n",
      "4th fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\py\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Model Evaluation:\n",
      "MAE: 6.722734504932056\n",
      "MSE: 97.82756970701668\n",
      "RMSE: 9.890782057401562\n",
      "R^2: 0.9167219594320943\n",
      "5th fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\py\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Model Evaluation:\n",
      "MAE: 6.8386423333647866\n",
      "MSE: 101.34241287670314\n",
      "RMSE: 10.06689688417951\n",
      "R^2: 0.9143157427403901\n",
      "Mean MAE : 6.839839163933914\n",
      "Mean MSE : 102.4179108036395\n",
      "Mean RMSE : 10.118141131925814\n",
      "Mean R2 : 0.9140790165931705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "def lgb_exec_kfold(preprocess_X, preprocess_Y, folds=5):\n",
    "    lgb = LGBMRegressor(random_state=777)\n",
    "    kfold = KFold(n_splits=folds)\n",
    "    evals = {'MAE': [], 'MSE': [], 'RMSE': [], 'R2': []}\n",
    "    lgb_mean_evals = {}\n",
    "\n",
    "    for iter_count, (train_index, test_index) in enumerate(kfold.split(preprocess_X)):\n",
    "        print(str(iter_count + 1) + \"th fold\")\n",
    "        X_train, X_test = preprocess_X.values[train_index], preprocess_X.values[test_index]\n",
    "        y_train, y_test = preprocess_Y.values[train_index], preprocess_Y.values[test_index]\n",
    "\n",
    "        lgb.fit(X_train, y_train)\n",
    "        lgb_predictions = lgb.predict(X_test)\n",
    "\n",
    "        lgb_mae, lgb_mse, lgb_rmse, lgb_r2 = lgb_regression_eval(y_test, lgb_predictions)\n",
    "\n",
    "        evals['MAE'].append(lgb_mae)\n",
    "        evals['MSE'].append(lgb_mse)\n",
    "        evals['RMSE'].append(lgb_rmse)\n",
    "        evals['R2'].append(lgb_r2)\n",
    "\n",
    "    lgb_mean_evals['CV'] = 'KFOLD = ' + str(folds) + ' (mean)'\n",
    "    for key, value in evals.items():\n",
    "        lgb_mean_evals[key] = np.mean(value)\n",
    "        print(\"Mean\", key, \":\", lgb_mean_evals[key])\n",
    "\n",
    "    return lgb_mean_evals\n",
    "\n",
    "# Call exec_kfold\n",
    "lgb_mean_evals = lgb_exec_kfold(preprocess_X, preprocess_Y, folds=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No Cross Validation vs Cross Validation 평가 지표 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_evals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m xgb_df_evals \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(xgb_evals,index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m xgb_df_evals\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_evals' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xgb_df_evals = pd.DataFrame(xgb_evals,index=[0])\n",
    "xgb_df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_df_evals = pd.DataFrame(lgb_evals,index=[0])\n",
    "lgb_df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_df_mean_evals= pd.DataFrame(xgb_mean_evals,index=[0])\n",
    "xgb_df_mean_evals = xgb_df_mean_evals[['CV','MAE','MSE','RMSE','R2']]\n",
    "xgb_df_mean_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_df_mean_evals= pd.DataFrame(lgb_mean_evals,index=[0])\n",
    "lgb_df_mean_evals = lgb_df_mean_evals[['CV','MAE','MSE','RMSE','R2']]\n",
    "lgb_df_mean_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchcv 하이퍼 파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최적 하이퍼 파라미터: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 30, 'num_leaves': 127}\n",
      "GridSearchCV 최고 정확도: -7.8595\n",
      "테스트 세트에서의 LGBMRegressor MAE: 7.820438777803821\n",
      "GridSearchCV 최적 하이퍼 파라미터: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 30, 'num_leaves': 127}\n",
      "GridSearchCV 최고 정확도: -130.0287\n",
      "테스트 세트에서의 LGBMRegressor MSE: 125.61350259089865\n",
      "GridSearchCV 최적 하이퍼 파라미터: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 30, 'num_leaves': 127}\n",
      "GridSearchCV 최고 정확도: -11.4026\n",
      "테스트 세트에서의 LGBMRegressor RMSE: 11.207742974876728\n",
      "GridSearchCV 최적 하이퍼 파라미터: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 30, 'num_leaves': 127}\n",
      "GridSearchCV 최고 정확도: 0.8904\n",
      "테스트 세트에서의 LGBMRegressor R2: 0.896455159603022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "lgb = LGBMRegressor(random_state=777)\n",
    "\n",
    "preprocess_X = pd.read_csv('preprocessed_data//preprocess_X_data.csv', index_col=0)\n",
    "preprocess_Y = pd.read_csv('preprocessed_data//preprocess_Y_data.csv', index_col=0) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocess_X, preprocess_Y, test_size=0.2, random_state=777)\n",
    "\n",
    "scorings = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'r2']\n",
    "param_grid = { \n",
    "        \"n_estimators\": [10, 20, 30],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "        \"num_leaves\": [31, 63, 127],\n",
    "        }\n",
    "\n",
    "for scoring in scorings:\n",
    "    grid_lgb = GridSearchCV(lgb, param_grid=param_grid, scoring=scoring, cv=5)\n",
    "    grid_lgb.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    print('GridSearchCV 최적 하이퍼 파라미터:', grid_lgb.best_params_)\n",
    "    print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_lgb.best_score_))\n",
    "    best_lgb = grid_lgb.best_estimator_\n",
    "\n",
    "    # GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행.\n",
    "    y_pred = best_lgb.predict(X_test)\n",
    "\n",
    "    if scoring == 'neg_mean_absolute_error':\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        print('테스트 세트에서의 LGBMRegressor MAE:', mae)\n",
    "    elif scoring == 'neg_mean_squared_error':\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print('테스트 세트에서의 LGBMRegressor MSE:', mse)\n",
    "    elif scoring == 'neg_root_mean_squared_error':\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        print('테스트 세트에서의 LGBMRegressor RMSE:', rmse)\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print('테스트 세트에서의 LGBMRegressor R2:', r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
